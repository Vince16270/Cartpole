{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Reinforcement Learning Experiment\n",
    "\n",
    "Dit notebook combineert documentatie, experimenten en visualisaties voor een Q-learning agent die getraind wordt in de CartPole-omgeving. Hieronder leg je de opzet, de trainingsprocedure en de interpretatie van de resultaten vast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importeren en Initialiseren\n",
    "\n",
    "We importeren de benodigde modules en initialiseren de omgeving en de agent. De omgeving wordt hierbij ingepakt via een wrapper in `environment.py` en de agent is ge√Ømplementeerd in `agent.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agent import QLearningAgent\n",
    "from environment import CartPoleEnvironment\n",
    "from utils import plot_rewards\n",
    "\n",
    "# Initialiseer de omgeving via de wrapper (environment.py)\n",
    "env_wrapper = CartPoleEnvironment('CartPole-v1')\n",
    "env = env_wrapper.env\n",
    "\n",
    "# Initialiseer de Q-learning agent\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "# Print een korte beschrijving van de gebruikte hyperparameters\n",
    "print(f\"Initial epsilon: {agent.epsilon}\")\n",
    "print(f\"Learning rate (alpha): {agent.alpha}\")\n",
    "print(f\"Discount factor (gamma): {agent.gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsloop\n",
    "\n",
    "In onderstaande cellen wordt de trainingsloop uitgevoerd. De agent wordt getraind over een vastgesteld aantal episodes, en per episode wordt de totale reward vastgelegd."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "num_episodes = 300\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env_wrapper.reset()\n",
    "    state_disc = agent.discretize(state)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(state_disc)\n",
    "        next_state, reward, done, truncated, info = env_wrapper.step(action)\n",
    "        next_state_disc = agent.discretize(next_state)\n",
    "        agent.update(state_disc, action, reward, next_state_disc, done)\n",
    "        state_disc = next_state_disc\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    agent.decay_epsilon()\n",
    "    reward_list.append(total_reward)\n",
    "    print(f\"Episode {episode+1}/{num_episodes}: Reward {total_reward}\")\n",
    "\n",
    "env_wrapper.close()\n",
    "\n",
    "# Plot en sla de reward-curve op\n",
    "plot_rewards(reward_list, title=\"Training Rewards over Episodes\", filename=\"train_rewards.png\")\n",
    "\n",
    "# Toon de gegenereerde grafiek\n",
    "img = plt.imread(\"train_rewards.png\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultaten en Analyse\n",
    "\n",
    "Bovenstaande grafiek toont de totale rewards per episode. Deze grafiek geeft een indicatie van hoe snel de agent leert de pole in balans te houden, en laat tevens de variabiliteit tussen episodes zien.\n",
    "\n",
    "### Mogelijke Observaties\n",
    "- Een stijgende trend in de reward-curve duidt op leerprogressie.\n",
    "- Grote schommelingen kunnen wijzen op een te hoge epsilon-waarde (te veel exploratie) of andere onstabiele hyperparameters.\n",
    "\n",
    "Door te experimenteren met hyperparameters zoals `alpha`, `gamma` en `epsilon_decay` kan de prestatie van de agent verder worden geoptimaliseerd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "\n",
    "Dit notebook dient als een interactief document voor zowel de documentatie als de visualisatie van de trainingsresultaten. Verdere analyses en experimenten kunnen worden uitgevoerd door extra metrieken toe te voegen en de huidige configuratie aan te passen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}